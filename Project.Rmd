---
title: "Project - STAT 151A"
author: "Aditya Jhanwar"
date: "12/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
setwd("~/151A/Project/")
rm(list=ls())
library(tidyverse)
library(gridExtra)
library(car)
library(MASS)
```

```{r load_data}
load(url("http://www.stat.berkeley.edu/users/nolan/data/baseball2012.rda"))
baseball = as_tibble(baseball)
```

# Data Exploration and Feature Creation 

### 1)

The first step is to clean the `baseball` data by removing unecessary explanatory variables and entries missing a `salary (observed Yi)` value.

```{r cleaning.data}
baseball = baseball %>% dplyr::select(-c("ID", "yearID", "teamID", "lgID", "nameFirst", "nameLast", "G_batting")) # remove unecessary variables
baseball = baseball %>% drop_na(salary) # remove units with no salary values
```

Next, I followed the author's process in creating new features as decribed in the textbook.

```{r derived.vars}
baseball = baseball %>% mutate(AVG = CH/CAB,
                               OBP = 100*(CH + CBB)/(CAB + CBB),
                               CAB.avg  = CAB/years,
                               CH.avg   = CH/years,
                               CHR.avg  = CHR/years,
                               CR.avg   = CR/years,
                               CRBI.avg = CRBI/years)
```

Finally, I cleaned the `Position` and `Years` explanatory variables through reimpementing them as dummy variables.

According to Fox's description of his analysis, he mentions **middle infielders** as players who consistently played second base or shortstop so I classified all individuals with either position as such. 

```{r dummy.vars.position}
new.pos = c()
MI = c("12", "1S", "23", "2B", "2S", "3S", "O2", "SS")
C  = c("C", "C1", "OC")
CF = c("CF")
  
# Assign new factor assignment for MI (middle infielders), C (catcher), CF (center field), and O (other)
for (i in baseball$POS){
  if      (i %in% MI) { new.pos = c(new.pos, "MI") }
  else if (i %in% C)  { new.pos = c(new.pos, "C")  }
  else if (i %in% CF) { new.pos = c(new.pos, "CF") }
  else                { new.pos = c(new.pos, "O")  }
}

baseball$POS = relevel(factor(new.pos), "O")
```

```{r dummy.vars.salary}
new.years = c()
neg.cont  = 6
neg.sal   = 3

for (i in baseball$years){
  if      (i >= neg.cont) { new.years = c(new.years, ".cont") }
  else if (i >= neg.sal)  { new.years = c(new.years, ".sal")   }
  else                    { new.years = c(new.years, "other")        }
}

baseball$neg = relevel(factor(new.years), "other")
```

```{r final.data}
lm.fit = lm(salary ~ ., data = baseball)
new.baseball = as_tibble(model.matrix(lm.fit)[,-1])
new.baseball$salary = baseball$salary
```

\newpage

Now that we have completed the feature creation process, the next step is to analyze the data itself. 

Firstly, I'll look at the the structure of the data itself and how the different variables are associated with each other. Since there are a lot of explanatory variables within the data, I will select a few key variables I believe to be the most influential in the model and investigate the structure.

**Note:**
- `G.x` = Position played at specified position
- `InnOut` = Time played in the field expressed as outs
- `PO` = Putouts
- `E` = Errors
- `CAB` = Career at bats
- `CR` = Career runs
- `CRBI` = Career runs batted in

```{r pairs.plot}
pairs(salary ~ G.x + InnOuts + PO + E + CAB + CR + CRBI, data=new.baseball)
```

From observing the paired structures of data it is evident that some features are uncorrelated whereas others are strongly correlated. However, this is mostly expected as certain features relate to one another. For example, a player's career at bats would be associated with his career runs or career runs batted in since all tie into a players capability of scoring bases.

This indicates a possible issue in inference of coefficients through linear modeling since the standard error calculation will be grossly inflated.

In addition, I noticed some of the variables have a stronger correlation with the salary than that of other variables. For example, `G.x` and `InnOuts` do not seem to have a strong association with `salary` whereas `CAB`, `CR`, and `CRBI` have comparitively stronger correlations with `salary`. This indicates some sort of variable selection and model pruning may be of benefit.

Next, I'd like to look into whether the data is distributed normally as per an assumption of guassian distributed errors in linear modelling. 

```{r salary.hist}
sal1 = ggplot(data=new.baseball, aes(x=salary)) + geom_histogram(bins=40) + ggtitle("Histogram of Salary")
sal2 = ggplot(data=new.baseball, aes(y=salary)) + geom_boxplot() + ggtitle("Boxplot of Salary")
grid.arrange(sal1, sal2, nrow=1)
```

The histogram above shows that the observed outcome values are not distributed normally at all. Hence, some sort of transformation of the data is necessary in order to use linear modelling.

```{r log.salary.hist}
log.sal1 = ggplot(data=new.baseball, aes(x=log(salary))) + geom_histogram(bins=40) + ggtitle("Histogram of log(Salary)")
log.sal2 = ggplot(data=new.baseball, aes(y=log(salary))) + geom_boxplot() +  ggtitle("Boxplot of log(Salary)")
grid.arrange(log.sal1, log.sal2, nrow=1)
```

Fox mentions log transforming the `salary` data in his linear modelling analysis and this is in line with the observed histograms above. The histogram of the original salaries is right skewed whereas the histogram of the log transformed salaries is somewhat more stabilized and appears more so normally distributed in some sense.

Fox also suggests log transforming some feature variables (years in the majors, career-at-bats) through preliminary examination and so I will do the same to carry forward analysis in a similar manner. An argument for why this may be benefificial is that it might garner a stronger linear relationship between the seemingly most influential explanatory variables and the salary and thus improving the model's predictive capability overall.

```{r cab.vs.salary.plot}
cab.plot1 = ggplot(data=new.baseball, aes(x=CAB, y=log(years))) + geom_point() + ggtitle(label = "Career-At-Bats vs Log(Salary)")
cab.plot2 = ggplot(data=new.baseball, aes(x=log(CAB), y=log(years))) + geom_point() + ggtitle(label = "Log(Career-At-Bats) vs Log(Salary)")
grid.arrange(cab.plot1, cab.plot2, nrow=1)
```

```{r years.vs.salary.plot}
yrs.plot1 = ggplot(data=new.baseball, aes(x=years, y=log(years))) + geom_point() + ggtitle(label = "Years in Majors vs Log(Salary)")
yrs.plot2 = ggplot(data=new.baseball, aes(x=log(years), y=log(years))) + geom_point() + ggtitle(label = "Log(Years in Majors) vs Log(Salary)")
grid.arrange(yrs.plot1, yrs.plot2, nrow=1)
```

```{r log.transform.features}
new.baseball$log.CAB    = log(new.baseball$CAB)
new.baseball$log.years  = log(new.baseball$years)
new.baseball$log.salary = log(new.baseball$salary)
new.baseball = new.baseball %>% dplyr::select(-c(CAB, years, salary))
```


\newpage

# Data Analysis 

### 1)

For the first of the project, I will be fitting a simple model that predicts `log(salary)` from the **dummy variables** for `years in majors` and `log(career runs)`, allowing for an interactio between the feature variables.

```{r simple.model}
dat1 = new.baseball %>% dplyr::select(log.salary, CR, neg.sal, neg.cont)
simple.model = lm(log.salary ~ log(1+CR)*(neg.cont + neg.sal), data=dat1)
simple.model
```

Although I have fitted the simple model above, I want to check for any outliers, high leverage points, and influential observations for further evaulation of the simple model. All criterions in determining such observations will be in line with what Fox suggests using.

```{r simple.model.infl}
hat.vals = hatvalues(simple.model)
stud.res = studres(simple.model)
cook.dis = cooks.distance(simple.model)
```

First, I'd like to take a look at the **high leverage** points, which are observations with explanatory variables markedly different from that of the average. In terms of numerical cutoffs for diagnostic statistics, _hat values exceeding **twice** the average hat value_ `(k+1)/n` _are noteworthy._

```{r}
h.3 = 3*length(simple.model$coefficients)/nrow(new.baseball)
high.leverage = measures[hat.vals > h.3,]
high.leverage
```

There appears to be **_17_** data points which have a relatively high leverage. 

In addition to high leverage points, I'll analyze discrepant observations to detect outliers within the data through utilizing **studentized residuals** with a numerical cutoff of `|t-test statistic| > 2`

```{r}
outliers = measures[abs(stud.res) > 2,]
outliers %>% head(n=5)
```

There are **_26_** observations which are determined to be outliers. 

Although I have determined observations that have high leverage or are outliers, what I am most conerned about are the subset of these points which have an influence on the determined coefficients of the model. Such points greatly alter the predictive capability of the simple model and thus cannot be overlooked.

Through recommendation by Fox, the criterion I will be using to determine highly influential points is `D_i > 4/(n-k-1)`

```{r}
cook.cutoff = 4/(nrow(new.baseball)-length(simple.model$coefficients))
influential.points = measures[cook.dis > cook.cutoff,]
influential.points %>% arrange(desc(cook.dis))
```

It appears there are **_16_** influential points within the dataset. I'm not very surprised as players' baseball data is incredibly varied and prone to uniquely performing individuals, thus causing there to be influential observations.

To better grasp the idea behind the information produced above, the following is a plot of the `hat` values representing the leverage with relation to the `studentized residuals`. Each circle represents an obervation with it's area proportional to it's calculated `Cook's Distance`.

**Note**: The horizontal line represents `3 times the average hat value` and the 2 vertical lines mark `t-test statistics of -2 and 2`.

```{r simple.model.hat.vs.studres}
dat = tibble(Hat.Values=hat.vals, Studentized.Residuals=stud.res, Cooks.Distance=cook.dis)

ggplot(aes(x=Hat.Values, y=Studentized.Residuals, size=Cooks.Distance), data=dat) + 
  geom_point(alpha=0.4) + scale_size(range=c(0, 15)) + 
  geom_vline(xintercept = 3*6/421, color='red', alpha=.7, linetype = "dashed") +
  geom_hline(yintercept = -2, color='red', alpha=.7, linetype = "dashed") + 
  geom_hline(yintercept = 2, color='red', alpha=.7, linetype = "dashed")
```











