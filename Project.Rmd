---
title: "Project - STAT 151A"
author: "Aditya Jhanwar"
date: "12/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
setwd("~/151A/Project/")
rm(list=ls())
set.seed(123)

library(tidyverse)
library(gridExtra)
library(car)
library(MASS)
library(leaps)
library(caret)
library(glmnet)
```

```{r load_data}
load(url("http://www.stat.berkeley.edu/users/nolan/data/baseball2012.rda"))
baseball = as_tibble(baseball)
```

# Data Exploration and Feature Creation 

### 1)

The first step is to clean the `baseball` data by removing unecessary explanatory variables and entries missing a `salary (observed Yi)` value.

```{r cleaning.data}
baseball = baseball %>% dplyr::select(-c("ID", "yearID", "teamID", "lgID", "nameFirst", "nameLast", "G_batting")) # remove unecessary variables
baseball = baseball %>% drop_na(salary) # remove units with no salary values
```

Next, I followed the author's process in creating new features as decribed in the textbook.

```{r derived.vars}
baseball = baseball %>% mutate(AVG = CH/CAB,
                               OBP = 100*(CH + CBB)/(CAB + CBB),
                               CAB.avg  = CAB/years,
                               CH.avg   = CH/years,
                               CHR.avg  = CHR/years,
                               CR.avg   = CR/years,
                               CRBI.avg = CRBI/years)
```

Finally, I cleaned the `Position` and `Years` explanatory variables through reimpementing them as dummy variables.

According to Fox's description of his analysis, he mentions **middle infielders** as players who consistently played second base or shortstop so I classified all individuals with either position as such. 

```{r dummy.vars.position}
new.pos = c()
MI = c("12", "1S", "23", "2B", "2S", "3S", "O2", "SS")
C  = c("C", "C1", "OC")
CF = c("CF")
  
# Assign new factor assignment for MI (middle infielders), C (catcher), CF (center field), and O (other)
for (i in baseball$POS){
  if      (i %in% MI) { new.pos = c(new.pos, "MI") }
  else if (i %in% C)  { new.pos = c(new.pos, "C")  }
  else if (i %in% CF) { new.pos = c(new.pos, "CF") }
  else                { new.pos = c(new.pos, "O")  }
}

baseball$POS = relevel(factor(new.pos), "O")
```

```{r dummy.vars.salary}
new.years = c()
neg.cont  = 6
neg.sal   = 3

for (i in baseball$years){
  if      (i >= neg.cont) { new.years = c(new.years, ".cont") }
  else if (i >= neg.sal)  { new.years = c(new.years, ".sal")   }
  else                    { new.years = c(new.years, "other")        }
}

baseball$neg = relevel(factor(new.years), "other")
```

```{r final.data}
lm.fit = lm(salary ~ ., data = baseball)
new.baseball = as_tibble(model.matrix(lm.fit)[,-1])
new.baseball$salary = baseball$salary
```

\newpage

Now that we have completed the feature creation process, the next step is to analyze the data itself. 

Firstly, I'll look at the the structure of the data itself and how the different variables are associated with each other. Since there are a lot of explanatory variables within the data, I will select a few key variables I believe to be the most influential in the model and investigate the structure.

**Note:**
- `G.x` = Position played at specified position
- `InnOut` = Time played in the field expressed as outs
- `PO` = Putouts
- `E` = Errors
- `CAB` = Career at bats
- `CR` = Career runs
- `CRBI` = Career runs batted in

```{r pairs.plot}
pairs(salary ~ G.x + InnOuts + PO + E + CAB + CR + CRBI, data=new.baseball)
```

From observing the paired structures of data it is evident that some features are uncorrelated whereas others are strongly correlated. However, this is mostly expected as certain features relate to one another. For example, a player's career at bats would be associated with his career runs or career runs batted in since all tie into a players capability of scoring bases.

This indicates a possible issue in inference of coefficients through linear modeling since the standard error calculation will be grossly inflated.

In addition, I noticed some of the variables have a stronger correlation with the salary than that of other variables. For example, `G.x` and `InnOuts` do not seem to have a strong association with `salary` whereas `CAB`, `CR`, and `CRBI` have comparitively stronger correlations with `salary`. This indicates some sort of variable selection and model pruning may be of benefit.

Next, I'd like to look into whether the data is distributed normally as per an assumption of guassian distributed errors in linear modelling. 

```{r salary.hist}
sal1 = ggplot(data=new.baseball, aes(x=salary)) + geom_histogram(bins=40) + ggtitle("Histogram of Salary")
sal2 = ggplot(data=new.baseball, aes(y=salary)) + geom_boxplot() + ggtitle("Boxplot of Salary")
grid.arrange(sal1, sal2, nrow=1)
```

The histogram above shows that the observed outcome values are not distributed normally at all. Hence, some sort of transformation of the data is necessary in order to use linear modelling.

```{r log.salary.hist}
log.sal1 = ggplot(data=new.baseball, aes(x=log(salary))) + geom_histogram(bins=40) + ggtitle("Histogram of log(Salary)")
log.sal2 = ggplot(data=new.baseball, aes(y=log(salary))) + geom_boxplot() +  ggtitle("Boxplot of log(Salary)")
grid.arrange(log.sal1, log.sal2, nrow=1)
```

Fox mentions log transforming the `salary` data in his linear modelling analysis and this is in line with the observed histograms above. The histogram of the original salaries is right skewed whereas the histogram of the log transformed salaries is somewhat more stabilized and appears more so normally distributed in some sense.

Fox also suggests log transforming some feature variables (years in the majors, career-at-bats) through preliminary examination and so I will do the same to carry forward analysis in a similar manner. An argument for why this may be benefificial is that it might garner a stronger linear relationship between the seemingly most influential explanatory variables and the salary and thus improving the model's predictive capability overall.

```{r cab.vs.salary.plot}
cab.plot1 = ggplot(data=new.baseball, aes(x=CAB, y=log(years))) + geom_point() + ggtitle(label = "Career-At-Bats vs Log(Salary)")
cab.plot2 = ggplot(data=new.baseball, aes(x=log(CAB), y=log(years))) + geom_point() + ggtitle(label = "Log(Career-At-Bats) vs Log(Salary)")
grid.arrange(cab.plot1, cab.plot2, nrow=1)
```

```{r years.vs.salary.plot}
yrs.plot1 = ggplot(data=new.baseball, aes(x=years, y=log(years))) + geom_point() + ggtitle(label = "Years in Majors vs Log(Salary)")
yrs.plot2 = ggplot(data=new.baseball, aes(x=log(years), y=log(years))) + geom_point() + ggtitle(label = "Log(Years in Majors) vs Log(Salary)")
grid.arrange(yrs.plot1, yrs.plot2, nrow=1)
```

```{r log.transform.features}
new.baseball$log.CAB    = log(new.baseball$CAB)
new.baseball$log.years  = log(new.baseball$years)
new.baseball$log.salary = log(new.baseball$salary)
new.baseball = new.baseball %>% dplyr::select(-c(CAB, years, salary))
```


\newpage

# Data Analysis 

### 1)

For the first of the project, I will be fitting a simple model that predicts `log(salary)` from the **dummy variables** for `years in majors` and `log(career runs)`, allowing for an interactio between the feature variables.

```{r simple.model}
dat1 = new.baseball %>% dplyr::select(log.salary, CR, neg.sal, neg.cont)
simple.model = lm(log.salary ~ log(1+CR)*(neg.cont + neg.sal), data=dat1)
simple.model
```

\newpage

### 2)

Although I have fitted the simple model above, I want to check for any outliers, high leverage points, and influential observations for further evaulation of the simple model. All criterions in determining such observations will be in line with what Fox suggests using.

```{r simple.model.measures}
hat.vals = hatvalues(simple.model)
stud.res = studres(simple.model)
cook.dis = cooks.distance(simple.model)

measures = tibble(Hat.Values=hat.vals, Studentized.Residuals=stud.res, Cooks.Distance=cook.dis)
```

First, I'd like to take a look at the **high leverage** points, which are observations with explanatory variables markedly different from that of the average. In terms of numerical cutoffs for diagnostic statistics, _hat values exceeding **twice** the average hat value_ `(k+1)/n` _are noteworthy._

```{r simple.model.infl}
h.3 = 3*length(simple.model$coefficients)/nrow(new.baseball)
high.leverage = measures[hat.vals > h.3,]
high.leverage
```

There appears to be **_17_** data points which have a relatively high leverage. 

In addition to high leverage points, I'll analyze discrepant observations to detect outliers within the data through utilizing **studentized residuals** with a numerical cutoff of `|t-test statistic| > 2`

```{r simple.model.outlier}
outliers = measures[abs(stud.res) > 2,]
outliers %>% head(n=5)
```

There are **_26_** observations which are determined to be outliers. 

Although I have determined observations that have high leverage or are outliers, what I am most conerned about are the subset of these points which have an influence on the determined coefficients of the model. Such points greatly alter the predictive capability of the simple model and thus cannot be overlooked.

Through recommendation by Fox, the criterion I will be using to determine highly influential points is `D_i > 4/(n-k-1)`

```{r simple.model.influential}
cook.cutoff = 4/(nrow(new.baseball)-length(simple.model$coefficients))
influential.points = measures[cook.dis > cook.cutoff,]
influential.points %>% arrange(desc(Cooks.Distance))
```

It appears there are **_16_** influential points within the dataset. I'm not very surprised as players' baseball data is incredibly varied and prone to uniquely performing individuals, thus causing there to be influential observations.

To better grasp the idea behind the information produced above, the following is a plot of the `hat` values representing the leverage with relation to the `studentized residuals`. Each circle represents an obervation with it's area proportional to it's calculated `Cook's Distance`.

**Note**: The horizontal line represents `3 times the average hat value` and the 2 vertical lines mark `t-test statistics of -2 and 2`.

```{r simple.model.hat.vs.studres}
measures = tibble(Hat.Values=hat.vals, Studentized.Residuals=stud.res, Cooks.Distance=cook.dis)

ggplot(aes(x=Hat.Values, y=Studentized.Residuals, size=Cooks.Distance), data=measures) + 
  geom_point(alpha=0.4) + scale_size(range=c(0, 15)) + 
  geom_vline(xintercept = 3*6/421, color='red', alpha=.7, linetype = "dashed") +
  geom_hline(yintercept = -2, color='red', alpha=.7, linetype = "dashed") + 
  geom_hline(yintercept = 2, color='red', alpha=.7, linetype = "dashed")
```

Lastly, I'd like to take a look at these influential points and observe the feature values.

```{r simple.model.infl.points}
new.baseball[cook.dis > cook.cutoff,]
```

Through manual obervation of the feature vaues of these influential points it appears that these observation have much higher or lower values for certain features such as `G.x`, `InnOuts`, `PO` compared to those of non-influential points. The table of encompassing Cook's Distance with Leverage Hat Value and Studentized Residuals do a much better job providing insight into why a point is influential, however. 

\newpage

### 3)

```{r full.fit}
all.fit = lm(log.salary ~ ., data = new.baseball)
summary(all.fit)
```

From former analysis I am aware of collinearity within the explanatory variables and hence inflated standard errors. However, it appears several of the coefficients are statistically significant and that the omnibus F-statistic for all explanatory variables in the model having coefficients of 0 is also statistically significant. In addition, the adjusted R-squared value is significantly lower than R-squared. This all leans toward the argument of possible variable selection that may improve the model's regression ability.

\newpage

### 4)

Next, I'll be finding the best **10** models for each model size using forward selection.

```{r top10.subset}
best10 = regsubsets(log.salary ~ ., data=new.baseball, nvmax=43, nbest=10, method="forward")
summary(best10)
```

\newpage

### 5)

```{r top5.models.bic}
lowest5.bic  = sort(summary(best10)$bic)[1:5]
top5.bic.loc = order(summary(best10)$bic)[1:5]
top5.models  = summary(best10)$which[top5.bic.loc,]
models = c()

for(i in 1:nrow(top5.models)){
  cat(sprintf("Model %d:", i))
  cat(sprintf("\nBIC: %f", lowest5.bic[i]))
  
  models = c(models, list(names(which(top5.models[i,]))[-1]))
  cat(sprintf("\nFeature: %s", models[[i]]))
  cat("\n\n")
}
```

\newpage

### 6)

Using `BIC` as a criterion may not be sufficient and hence I will use 10-fold cross validation to re-rank the 5 models determined above. Rather than hard-code the k-fold procedure, I will be using built-in functions available in the `caret` package.

The output will show the features of each model for clarity as well as the previously determined BIC value and the newly determined mean-MSE (through 10-fold CV).

```{r 10.fold.cv}
# 10-fold cross validation setup
train.control = trainControl(method = "cv", number = 10)
mean.mse = c()

for (i in 1:5){
  model.to.train.test  = as.formula(paste("log.salary ~", paste(models[[i]], collapse=" + ")))
  model.trained.tested = train(model.to.train.test, data = new.baseball, method = "lm", trControl = train.control) 
  mean.mse =  c(mean.mse, model.trained.tested$results[[2]]**2)

  cat(sprintf("Model %d:", i))
  cat(sprintf("\nBIC: %f", lowest5.bic[i]))
  cat(sprintf("\nFeature: %s", models[[i]]))
  cat(sprintf("\n--> MSE: %f", mean.mse[i]))
  cat(sprintf("\n\n"))
}
```



\newpage

### 7) 

In determining the value of the hyperparameter `lambda` to use for the LASSO model (original feature variables), I will be using 10-fold cross validation with an average MSE for the criterion.

```{r}
X = new.baseball %>% dplyr::select(-c(log.salary)) %>% as.matrix()
y = new.baseball$log.salary

lasso.model    = glmnet(x = X, y = y, alpha = 1)
cv.lasso.model = cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10)
```

The following is the plot of the `log of the hyperparameter lambda` with respect to the resulting `average MSE`.

```{r}
plot(cv.lasso.model)
```

The left-most dashed vertical line in the plot represents the lambda corresponding to the minimum MSE whereas the right-most dashed vertical line represents the lambda 1 standard error away from the min lambda. 

I believe the data intrinsically contains a lot of collinear explanatory variables and would like to reduce the number of features used in model prediction. I believe the lambda corresponding to the minimum MSE would be suitable as it not only performs variable selection through utilizing just 28 feature variables for regression rather than all 43, it also has the lowest MSE out of all other possible lambda hyperparameter values and thus seems to be the best choice in prediction. 

The value of `lambda` I will use for the LASSO model is the following:

```{r}
best.lasso.lambda = cv.lasso.model$lambda.min
best.lasso.lambda
```

The following is a visualization of how different values of lambda influence the coefficient values of the regression model:

```{r}
plot(lasso.model, xvar = "lambda", )
lines(c(best.lasso.lambda, best.lasso.lambda), c(-1, 1), lty = "dashed", lwd = 3)
```

Lastly, using the hyperparameter value of lambda specified above, I'd like to analyze which coefficients are retained through implicit variable selection of LASSO.

```{r}
best.lasso.coefs = predict(lasso.model, type = 'coefficients', s = best.lasso.lambda)
best.lasso.coefs
```








